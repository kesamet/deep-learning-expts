{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bCoActnkniSV"
   },
   "source": [
    "# Seq2seq Learning - Keras\n",
    "\n",
    "- Unequal lengths\n",
    "- Equal lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2989,
     "status": "ok",
     "timestamp": 1548984527117,
     "user": {
      "displayName": "Kok Meng Tan",
      "photoUrl": "",
      "userId": "03184747090436452828"
     },
     "user_tz": -480
    },
    "id": "Xg3q_5yyq1Nu",
    "outputId": "41a03fae-c6fa-4a9b-fb12-8ce0a1cd622a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n",
      "TensorFlow Version: 1.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1440,
     "status": "ok",
     "timestamp": 1548984533200,
     "user": {
      "displayName": "Kok Meng Tan",
      "photoUrl": "",
      "userId": "03184747090436452828"
     },
     "user_tz": -480
    },
    "id": "UXLrXb4NniSX",
    "outputId": "a4b053e1-4030-4c05-cd3a-c7e4d1fa7acd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, RepeatVector\n",
    "from keras.layers import Conv1D, GlobalMaxPool1D, Flatten, Bidirectional, MaxPooling1D\n",
    "from keras.layers import Activation, TimeDistributed, Dropout, Embedding\n",
    "from keras import regularizers\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.models import Model, model_from_json\n",
    "from keras.layers import Input, Dense\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uk84lXHSsFwb"
   },
   "source": [
    "## Downloading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-aqcR9w_r77T"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4k0s2WgCniSe"
   },
   "source": [
    "## Unequal lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j_uwSCRJniSg"
   },
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = 'gdrive/My Drive/Colab Notebooks/data/fra.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LxNt-EWfniSj",
    "outputId": "9e61608a-0b23-40ec-9b5c-553534799d6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 71\n",
      "Number of unique output tokens: 94\n",
      "Max sequence length for inputs: 16\n",
      "Max sequence length for outputs: 59\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "    \n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BfWndDMnniSn"
   },
   "source": [
    "### Char by char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xLx1bVaaniSo"
   },
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3YWu5pAcniSr",
    "outputId": "866d3117-02f8-4cc4-aaee-4ddf9c3bbd45",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_35 (InputLayer)           (None, None, 71)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_36 (InputLayer)           (None, None, 94)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_34 (LSTM)                  [(None, 256), (None, 335872      input_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_35 (LSTM)                  [(None, None, 256),  359424      input_36[0][0]                   \n",
      "                                                                 lstm_34[0][1]                    \n",
      "                                                                 lstm_34[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, None, 94)     24158       lstm_35[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 719,454\n",
      "Trainable params: 719,454\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 59s 7ms/step - loss: 0.9220 - val_loss: 0.9748\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 54s 7ms/step - loss: 0.7278 - val_loss: 0.7913\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 54s 7ms/step - loss: 0.6180 - val_loss: 0.7018\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 54s 7ms/step - loss: 0.5645 - val_loss: 0.6596\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 57s 7ms/step - loss: 0.5241 - val_loss: 0.6385\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 55s 7ms/step - loss: 0.4910 - val_loss: 0.5958\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 54s 7ms/step - loss: 0.4645 - val_loss: 0.5663\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 54s 7ms/step - loss: 0.4423 - val_loss: 0.5534\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 55s 7ms/step - loss: 0.4236 - val_loss: 0.5354\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 57s 7ms/step - loss: 0.4055 - val_loss: 0.5266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f71b406ce80>"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "print(model.summary())\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10, #epochs,\n",
    "          validation_split=0.2)\n",
    "\n",
    "# Save model\n",
    "# model.save_weights('models/s2s.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fNtRMzWDniSv",
    "outputId": "e9fec9ff-b77a-4ce9-f7f7-7994c79e07c5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Allez vous prêtes !\n",
      "\n",
      "Target sentence: \tVa !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Arrête de la mainten !\n",
      "\n",
      "Target sentence: \tCours !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Arrête de la mainten !\n",
      "\n",
      "Target sentence: \tCourez !\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Arrête de la mainten !\n",
      "\n",
      "Target sentence: \tAu feu !\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: Arrête de la mais.\n",
      "\n",
      "Target sentence: \tÀ l'aide !\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: Viens partir.\n",
      "\n",
      "Target sentence: \tSaute.\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arrête de la mainten !\n",
      "\n",
      "Target sentence: \tÇa suffit !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arrête de la mainten !\n",
      "\n",
      "Target sentence: \tStop !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arrête de la mainten !\n",
      "\n",
      "Target sentence: \tArrête-toi !\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Attendez à l'ait !\n",
      "\n",
      "Target sentence: \tAttends !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(10):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)\n",
    "    print('Target sentence:', target_texts[seq_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JhRrKlX_niSz"
   },
   "source": [
    "### Word by word\n",
    "\n",
    "- find set of words\n",
    "- convert words to indices\n",
    "- create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KRotSodxniS1"
   },
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fx8b5UytniS5",
    "outputId": "ab993c83-70c1-4331-db58-17801b3e636e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_31 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_32 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, None, 256)    18176       input_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, None, 256)    24064       input_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_30 (LSTM)                  [(None, 256), (None, 525312      embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_31 (LSTM)                  [(None, None, 256),  525312      embedding_6[0][0]                \n",
      "                                                                 lstm_30[0][1]                    \n",
      "                                                                 lstm_30[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, None, 94)     24158       lstm_31[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,117,022\n",
      "Trainable params: 1,117,022\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_31 to have 2 dimensions, but got array with shape (10000, 16, 71)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-035eba7cbeb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m           validation_split=0.2)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf18/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1630\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1631\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf18/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m   1474\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1476\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1477\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1478\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf18/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    111\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_31 to have 2 dimensions, but got array with shape (10000, 16, 71)"
     ]
    }
   ],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "x = Embedding(num_encoder_tokens, latent_dim)(encoder_inputs)\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "_, state_h, state_c = encoder(x)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "x = Embedding(num_decoder_tokens, latent_dim)(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "x, _, _ = decoder_lstm(x, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(x)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "print(model.summary())\n",
    "\n",
    "# Compile & run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "# Note that `decoder_target_data` needs to be one-hot encoded,\n",
    "# rather than sequences of integers like `decoder_input_data`!\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5d8sARuyniS9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jeKlh-zhniTB",
    "outputId": "a67cf14a-7563-409c-fe9c-735db63ee994"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27, 8], [38, 20], [48, 29], [46, 20], [23], [24], [43, 29], [23, 38], [43, 20], [47, 7, 8, 20]]\n",
      "[[27  8  0  0]\n",
      " [38 20  0  0]\n",
      " [48 29  0  0]\n",
      " [46 20  0  0]\n",
      " [23  0  0  0]\n",
      " [24  0  0  0]\n",
      " [43 29  0  0]\n",
      " [23 38  0  0]\n",
      " [43 20  0  0]\n",
      " [47  7  8 20]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Flatten\n",
    "docs = ['Well done!',\n",
    "        'Good work',\n",
    "        'Great effort',\n",
    "        'nice work',\n",
    "        'Excellent!',\n",
    "        'Weak',\n",
    "        'Poor effort!',\n",
    "        'not good',\n",
    "        'poor work',\n",
    "        'Could have done better']\n",
    "# define class labels\n",
    "labels = np.array([1,1,1,1,1,0,0,0,0,0])\n",
    "# integer encode the documents\n",
    "vocab_size = 50\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(encoded_docs)\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DcwLimGUniTF",
    "outputId": "af7a606a-8845-45bc-ebbb-3365707ed2f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_45 (InputLayer)        (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_18 (Embedding)     (None, 4, 8)              400       \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 433\n",
      "Trainable params: 433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "inputs = Input(shape=(max_length,))\n",
    "x = Embedding(vocab_size, 8)(inputs)\n",
    "x = Flatten()(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs, outputs)\n",
    "print(model.summary())\n",
    "\n",
    "# fit the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.fit(padded_docs, labels, epochs=200, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JvehR43OniTJ",
    "outputId": "2dffadf5-4f73-43b3-c9cf-05d336027253"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7861039 ],\n",
       "       [0.72447515],\n",
       "       [0.69255024],\n",
       "       [0.768694  ],\n",
       "       [0.61293256],\n",
       "       [0.34747598],\n",
       "       [0.30596223],\n",
       "       [0.38281503],\n",
       "       [0.41705278],\n",
       "       [0.09242197]], dtype=float32)"
      ]
     },
     "execution_count": 78,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(padded_docs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PK5qhk3EniTO"
   },
   "outputs": [],
   "source": [
    "### another implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cj8qClUWniTU"
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model\n",
    "\n",
    " \n",
    "# generate a sequence of random integers\n",
    "def generate_sequence(length, n_unique):\n",
    "\treturn [randint(1, n_unique-1) for _ in range(length)]\n",
    " \n",
    "# prepare data for the LSTM\n",
    "def get_dataset(n_in, n_out, cardinality, n_samples):\n",
    "\tX1, X2, y = list(), list(), list()\n",
    "\tfor _ in range(n_samples):\n",
    "\t\t# generate source sequence\n",
    "\t\tsource = generate_sequence(n_in, cardinality)\n",
    "\t\t# define target sequence\n",
    "\t\ttarget = source[:n_out]\n",
    "\t\ttarget.reverse()\n",
    "\t\t# create padded input target sequence\n",
    "\t\ttarget_in = [0] + target[:-1]\n",
    "\t\t# encode\n",
    "\t\tsrc_encoded = to_categorical(source, num_classes=cardinality)\n",
    "\t\ttar_encoded = to_categorical(target, num_classes=cardinality)\n",
    "\t\ttar2_encoded = to_categorical(target_in, num_classes=cardinality)\n",
    "\t\t# store\n",
    "\t\tX1.append(src_encoded)\n",
    "\t\tX2.append(tar2_encoded)\n",
    "\t\ty.append(tar_encoded)\n",
    "\treturn np.array(X1), np.array(X2), np.array(y)\n",
    " \n",
    "# decode a one hot encoded string\n",
    "def one_hot_decode(encoded_seq):\n",
    "\treturn [np.argmax(vector) for vector in encoded_seq]\n",
    " \n",
    "# configure problem\n",
    "n_features = 50 + 1\n",
    "n_steps_in = 6\n",
    "n_steps_out = 3\n",
    "# generate a single source and target sequence\n",
    "X1, X2, y = get_dataset(n_steps_in, n_steps_out, n_features, 1)\n",
    "print(X1.shape, X2.shape, y.shape)\n",
    "print('X1=%s, X2=%s, y=%s' % (one_hot_decode(X1[0]), one_hot_decode(X2[0]), one_hot_decode(y[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h06IdvaHniTZ"
   },
   "outputs": [],
   "source": [
    "# returns train, inference_encoder and inference_decoder models\n",
    "def define_models(n_input, n_output, n_units):\n",
    "    # define training encoder\n",
    "    encoder_inputs = Input(shape=(None, n_input))\n",
    "    encoder = LSTM(n_units, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    # define training decoder\n",
    "    decoder_inputs = Input(shape=(None, n_output))\n",
    "    decoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "    decoder_dense = Dense(n_output, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    # define inference encoder\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "    # define inference decoder\n",
    "    decoder_state_input_h = Input(shape=(n_units,))\n",
    "    decoder_state_input_c = Input(shape=(n_units,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "    # return all models\n",
    "    return model, encoder_model, decoder_model\n",
    "\n",
    "# generate target given source sequence\n",
    "def predict_sequence(infenc, infdec, source, n_steps, cardinality):\n",
    "    # encode\n",
    "    state = infenc.predict(source)\n",
    "    # start of sequence input\n",
    "    target_seq = np.array([0.0 for _ in range(cardinality)]).reshape(1, 1, cardinality)\n",
    "    # collect predictions\n",
    "    output = list()\n",
    "    for t in range(n_steps):\n",
    "        # predict next char\n",
    "        yhat, h, c = infdec.predict([target_seq] + state)\n",
    "        # store prediction\n",
    "        output.append(yhat[0,0,:])\n",
    "        # update state\n",
    "        state = [h, c]\n",
    "        # update target sequence\n",
    "        target_seq = yhat\n",
    "    return np.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "va6c2oQnniTd"
   },
   "outputs": [],
   "source": [
    "# generate training dataset\n",
    "X1, X2, y = get_dataset(n_steps_in, n_steps_out, n_features, 100000)\n",
    "print(X1.shape, X2.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PCxX-7yBniTi"
   },
   "outputs": [],
   "source": [
    "# model\n",
    "train, infenc, infdec = define_models(n_features, n_features, 128)\n",
    "train.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "# train model\n",
    "train.fit([X1, X2], y, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wvo9IQUXniTl"
   },
   "outputs": [],
   "source": [
    "# evaluate LSTM\n",
    "total, correct = 100, 0\n",
    "for _ in range(total):\n",
    "\tX1, X2, y = get_dataset(n_steps_in, n_steps_out, n_features, 1)\n",
    "\ttarget = predict_sequence(infenc, infdec, X1, n_steps_out, n_features)\n",
    "\tif np.array_equal(one_hot_decode(y[0]), one_hot_decode(target)):\n",
    "\t\tcorrect += 1\n",
    "print('Accuracy: %.2f%%' % (float(correct)/float(total)*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s-wtpKpZniTn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "heXkOAPlniTq"
   },
   "source": [
    "## Equal lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vlZZS_bPniTr",
    "outputId": "f29a4114-c794-464d-e467-a65d176c3315"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_47 (InputLayer)        (None, 7, 12)             0         \n",
      "_________________________________________________________________\n",
      "lstm_42 (LSTM)               (None, 128)               72192     \n",
      "_________________________________________________________________\n",
      "repeat_vector_11 (RepeatVect (None, 4, 128)            0         \n",
      "_________________________________________________________________\n",
      "lstm_43 (LSTM)               (None, 4, 128)            131584    \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 4, 12)             1548      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 4, 12)             0         \n",
      "=================================================================\n",
      "Total params: 205,324\n",
      "Trainable params: 205,324\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "MAXLEN = 7\n",
    "chars = '0123456789+ '\n",
    "DIGITS = 3\n",
    "\n",
    "RNN = LSTM\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "LAYERS = 1\n",
    "\n",
    "print('Build model...')\n",
    "# model = Sequential()\n",
    "# # \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE.\n",
    "# # Note: In a situation where your input sequences have a variable length,\n",
    "# # use input_shape=(None, num_feature).\n",
    "# model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars))))\n",
    "# # As the decoder RNN's input, repeatedly provide with the last hidden state of\n",
    "# # RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
    "# # length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
    "# model.add(RepeatVector(DIGITS + 1))\n",
    "# # The decoder RNN could be multiple layers stacked or a single layer.\n",
    "# for _ in range(LAYERS):\n",
    "#     # By setting return_sequences to True, return not only the last output but\n",
    "#     # all the outputs so far in the form of (num_samples, timesteps,\n",
    "#     # output_dim). This is necessary as TimeDistributed in the below expects\n",
    "#     # the first dimension to be the timesteps.\n",
    "#     model.add(RNN(HIDDEN_SIZE, return_sequences=True))\n",
    "\n",
    "# # Apply a dense layer to the every temporal slice of an input. For each of step\n",
    "# # of the output sequence, decide which character should be chosen.\n",
    "# model.add(TimeDistributed(Dense(len(chars))))\n",
    "# model.add(Activation('softmax'))\n",
    "\n",
    "inputs = Input(shape=(MAXLEN, len(chars)))\n",
    "x = RNN(HIDDEN_SIZE)(inputs)\n",
    "x = RepeatVector(DIGITS + 1)(x)\n",
    "for _ in range(LAYERS):\n",
    "    x = RNN(HIDDEN_SIZE, return_sequences=True)(x)\n",
    "x = TimeDistributed(Dense(len(chars)))(x)\n",
    "outputs = Activation('softmax')(x)\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zge4-48_niTv"
   },
   "source": [
    "## Saving keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gc5L5lx2niTx"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from keras.models import model_from_json, load_model\n",
    "\n",
    "# Option 1: Save Weights + Architecture\n",
    "model.save_weights('model_weights.h5')\n",
    "with open('model_architecture.json', 'w') as f:\n",
    "    f.write(model.to_json())\n",
    "# Option 1: Load Weights + Architecture\n",
    "with open('model_architecture.json', 'r') as f:\n",
    "    new_model_1 = model_from_json(f.read())\n",
    "new_model_1.load_weights('model_weights.h5')\n",
    "\n",
    "# Option 2: Save/load the entire model\n",
    "model.save('my_model.h5')\n",
    "new_model_2 = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jw8TPrIjniTz"
   },
   "source": [
    "## Saving sklearn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HzT5cl4pniT0"
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "# Saving a model\n",
    "joblib.dump(my_model, '_model.pkl')\n",
    "\n",
    "# Loading a model\n",
    "loaded_model = joblib.load('my_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "keras_seq2seq_learning.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
