{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LvYaPYrn0S5n"
   },
   "source": [
    "# Doc classification\n",
    "\n",
    "* [Preprocessing](#preprocess)\n",
    "* [CNN](#CNN)\n",
    "* [Birdirectional LSTM](#BiLSTM)\n",
    "* [Attention GRU](#AttGRU)\n",
    "* [Hierarchical LSTM](#H-LSTM)\n",
    "* [Hierarchical Attention Networks](#HAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_6epf4f-xtRP"
   },
   "source": [
    "## Get glove_6B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zk6fbj9vi3UV"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "DIRNAME = 'gdrive/My Drive/Colab Notebooks/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fVisKlSVx0pO"
   },
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3092,
     "status": "ok",
     "timestamp": 1543199886642,
     "user": {
      "displayName": "Kok Meng Tan",
      "photoUrl": "",
      "userId": "03184747090436452828"
     },
     "user_tz": -480
    },
    "id": "-9Xzhx9rvN13",
    "outputId": "e4a3089a-9f8b-4717-d16e-21581e602b01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n",
      "TensorFlow Version: 1.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KGmO4EWb0S5p"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1086,
     "status": "ok",
     "timestamp": 1543199887849,
     "user": {
      "displayName": "Kok Meng Tan",
      "photoUrl": "",
      "userId": "03184747090436452828"
     },
     "user_tz": -480
    },
    "id": "jl_jB23ZuqaI",
    "outputId": "df57cfdd-96d4-41f7-deb9-c3b69b5c9170"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Concatenate, Dropout\n",
    "from keras.layers import Bidirectional, LSTM, GRU, TimeDistributed\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oLFBPH1z0S5t"
   },
   "source": [
    "<a id='preprocess'></a>\n",
    "\n",
    "## Preprocessing keras imdb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7727,
     "status": "ok",
     "timestamp": 1543200012712,
     "user": {
      "displayName": "Kok Meng Tan",
      "photoUrl": "",
      "userId": "03184747090436452828"
     },
     "user_tz": -480
    },
    "id": "9i0wwwtgu_jf",
    "outputId": "8f84a4ab-c74d-405a-d3e8-a3801dec7ef0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n",
      "... Padding sequences (samples x time)\n",
      "X_train shape: (25000, 250)\n",
      "X_test shape: (25000, 250)\n"
     ]
    }
   ],
   "source": [
    "def get_imdb_data(maxlen=100, max_features=20000):\n",
    "    from keras.datasets import imdb\n",
    "    from keras.preprocessing import sequence\n",
    "    \n",
    "    (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "    print(len(X_train), 'train sequences')\n",
    "    print(len(X_test), 'test sequences')\n",
    "    \n",
    "    print('... Padding sequences (samples x time)')\n",
    "    X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "    X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "    print('X_train shape:', X_train.shape)\n",
    "    print('X_test shape:', X_test.shape)\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "(x_train, y_train), (x_val, y_val) = get_imdb_data(MAX_SEQUENCE_LENGTH, MAX_NUM_WORDS)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JEvJiktXFXox"
   },
   "source": [
    "## Download imdb train from Kaggle\n",
    "wget https://www.kaggle.com/c/word2vec-nlp-tutorial/download/labeledTrainData.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 914
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 874,
     "status": "error",
     "timestamp": 1543200158822,
     "user": {
      "displayName": "Kok Meng Tan",
      "photoUrl": "",
      "userId": "03184747090436452828"
     },
     "user_tz": -480
    },
    "id": "EAHHKwd-0S5u",
    "outputId": "138527d0-c1b2-4f51-cb35-2f7518f7e9ce"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \"\", string)\n",
    "    string = re.sub(r\"\\'\", \"\", string)\n",
    "    string = re.sub(r\"\\\"\", \"\", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "data_train = pd.read_csv(DIRNAME + 'data/labeledTrainData.tsv', sep='\\t')\n",
    "texts = []\n",
    "labels = []\n",
    "for i in range(data_train.review.shape[0]):\n",
    "    text = BeautifulSoup(data_train.review[i], 'html5lib')\n",
    "    texts.append(clean_str(text.get_text()))\n",
    "    labels.append(data_train.sentiment[i])\n",
    "    \n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yU8I1zkf0S5x",
    "outputId": "43d58cb6-2bba-48b0-8cf1-36a50cdaddad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 81503 unique tokens.\n",
      "Shape of data tensor: (25000, 1000)\n",
      "Number of positive and negative reviews in traing and validation set \n",
      "[10023.  9977.]\n",
      "[2477. 2523.]\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "\n",
    "\n",
    "# Shuffling and splitting into train and validation sets\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n",
    "print('Number of positive and negative reviews in training and validation set ')\n",
    "print(y_train.sum(axis=0))\n",
    "print(y_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-xQyLIQfzxTI"
   },
   "source": [
    "## Glove embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I6JFqvLN0S5z",
    "outputId": "7896212e-dd9b-4eb0-f7c1-10bf4b09545e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors in Glove 6B 100d.\n"
     ]
    }
   ],
   "source": [
    "def glove_embedding_matrix(EMBEDDING_DIM, word_index):\n",
    "    embeddings_index = {}\n",
    "    with open(DIRNAME + 'data/glove.6B/glove.6B.100d.txt') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    print('Total %s word vectors in Glove 6B 100d.' % len(embeddings_index))\n",
    "    \n",
    "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "embedding_matrix = glove_embedding_matrix(EMBEDDING_DIM, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9HfARsQQ0S52"
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(\n",
    "    len(word_index) + 1,\n",
    "    EMBEDDING_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=MAX_SEQUENCE_LENGTH,\n",
    "    trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rrTZX71d0S54"
   },
   "source": [
    "<a id='CNN'></a>\n",
    "\n",
    "##  CNN - Yoo Kim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 878,
     "status": "error",
     "timestamp": 1542857139785,
     "user": {
      "displayName": "Kok Meng Tan",
      "photoUrl": "",
      "userId": "03184747090436452828"
     },
     "user_tz": -480
    },
    "id": "mMaxdbiJvrGB",
    "outputId": "266e2b0a-cd86-42a2-b548-58c4988c45e5"
   },
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH, ), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "kernel_sizes = [3,4,5]\n",
    "convs = []\n",
    "for fsz in kernel_sizes:\n",
    "    l_conv = Conv1D(filters=128, kernel_size=fsz, activation='relu')(embedded_sequences)\n",
    "    l_pool = MaxPooling1D(pool_size=5)(l_conv)\n",
    "    convs.append(l_pool)\n",
    "    \n",
    "l_merge = Concatenate(axis=1)(convs)\n",
    "l_cov1= Conv1D(filters=128, kernel_size=5, activation='relu')(l_merge)\n",
    "l_pool1 = MaxPooling1D(pool_size=5)(l_cov1)\n",
    "l_cov2 = Conv1D(filters=128, kernel_size=5, activation='relu')(l_pool1)\n",
    "l_pool2 = MaxPooling1D(pool_size=30)(l_cov2)\n",
    "l_flat = Flatten()(l_pool2)\n",
    "l_dense = Dense(units=128, activation='relu')(l_flat)\n",
    "preds = Dense(units=2, activation='softmax')(l_dense)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 463647,
     "status": "ok",
     "timestamp": 1535364185760,
     "user": {
      "displayName": "Kok Meng Tan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "113459036002015692798"
     },
     "user_tz": -480
    },
    "id": "hg2PuxUAxsUz",
    "outputId": "3a14e8ec-e400-423c-a22f-e7500ac4d146"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 73s 3ms/step - loss: 0.6939 - acc: 0.5546 - val_loss: 0.5648 - val_acc: 0.7056\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.56476, saving model to weights.h5\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 65s 3ms/step - loss: 0.4463 - acc: 0.8022 - val_loss: 0.2967 - val_acc: 0.8758\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.56476 to 0.29674, saving model to weights.h5\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 65s 3ms/step - loss: 0.2612 - acc: 0.8951 - val_loss: 0.3543 - val_acc: 0.8393\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.29674\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 65s 3ms/step - loss: 0.1968 - acc: 0.9251 - val_loss: 0.3229 - val_acc: 0.8709\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.29674\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 65s 3ms/step - loss: 0.1503 - acc: 0.9432 - val_loss: 0.3197 - val_acc: 0.8819\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.29674\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 65s 3ms/step - loss: 0.1095 - acc: 0.9584 - val_loss: 0.3619 - val_acc: 0.8754\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.29674\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 65s 3ms/step - loss: 0.0646 - acc: 0.9758 - val_loss: 0.3777 - val_acc: 0.8723\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.29674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff2a9d73198>"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 1024\n",
    "\n",
    "file_path = 'weights.h5'\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1,\n",
    "                             save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor='val_loss', mode='min', patience=5)\n",
    "\n",
    "callbacks_list = [checkpoint, early]\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          shuffle=True,\n",
    "          validation_data=(x_val, y_val),\n",
    "          callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k0H1Y-yBE6pD"
   },
   "source": [
    "### Customized IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E_NlX8VpvXhX"
   },
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH, ), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "kernel_sizes = [3,4,5]\n",
    "convs = []\n",
    "for fsz in kernel_sizes:\n",
    "    l_conv = Conv1D(filters=128, kernel_size=fsz, activation='relu')(embedded_sequences)\n",
    "    l_pool = MaxPooling1D(pool_size=5)(l_conv)\n",
    "    convs.append(l_pool)\n",
    "    \n",
    "l_merge = Concatenate(axis=1)(convs)\n",
    "l_cov1= Conv1D(filters=128, kernel_size=5, activation='relu')(l_merge)\n",
    "l_pool1 = MaxPooling1D(pool_size=5)(l_cov1)\n",
    "l_cov2 = Conv1D(filters=128, kernel_size=5, activation='relu')(l_pool1)\n",
    "l_pool2 = MaxPooling1D(pool_size=30)(l_cov2)\n",
    "l_flat = Flatten()(l_pool2)\n",
    "l_dense = Dense(units=128, activation='relu')(l_flat)\n",
    "preds = Dense(units=2, activation='softmax')(l_dense)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ousMoU1A0S55",
    "outputId": "82fb6ed4-c510-487f-9cdf-5e9414a64d0f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kokmeng/anaconda3/envs/tf18/lib/python3.5/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(filters=128, kernel_size=3, activation=\"relu\")`\n",
      "  \n",
      "/home/kokmeng/anaconda3/envs/tf18/lib/python3.5/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(filters=128, kernel_size=4, activation=\"relu\")`\n",
      "  \n",
      "/home/kokmeng/anaconda3/envs/tf18/lib/python3.5/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(filters=128, kernel_size=5, activation=\"relu\")`\n",
      "  \n",
      "/home/kokmeng/anaconda3/envs/tf18/lib/python3.5/site-packages/ipykernel_launcher.py:12: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - more complex convolutional neural network\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1000, 100)    8150400     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 998, 128)     38528       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 997, 128)     51328       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 996, 128)     64128       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 199, 128)     0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 199, 128)     0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 199, 128)     0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "merge_1 (Merge)                 (None, 597, 128)     0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 593, 128)     82048       merge_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 118, 128)     0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 114, 128)     82048       max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 3, 128)       0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 384)          0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          49280       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            258         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,518,018\n",
      "Trainable params: 8,518,018\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kokmeng/anaconda3/envs/tf18/lib/python3.5/site-packages/ipykernel_launcher.py:29: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 1200s 60ms/step - loss: 0.5701 - acc: 0.6884 - val_loss: 0.3720 - val_acc: 0.8310\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 1164s 58ms/step - loss: 0.3148 - acc: 0.8670 - val_loss: 0.3099 - val_acc: 0.8734\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 1167s 58ms/step - loss: 0.2335 - acc: 0.9092 - val_loss: 0.7432 - val_acc: 0.7486\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 1168s 58ms/step - loss: 0.1637 - acc: 0.9396 - val_loss: 0.2845 - val_acc: 0.8838\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 1168s 58ms/step - loss: 0.1027 - acc: 0.9636 - val_loss: 0.3486 - val_acc: 0.8786\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 1164s 58ms/step - loss: 0.0605 - acc: 0.9796 - val_loss: 0.5239 - val_acc: 0.8824\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 1168s 58ms/step - loss: 0.0385 - acc: 0.9873 - val_loss: 0.5099 - val_acc: 0.8804\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 1167s 58ms/step - loss: 0.0295 - acc: 0.9900 - val_loss: 0.5020 - val_acc: 0.8802\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 1160s 58ms/step - loss: 0.0262 - acc: 0.9916 - val_loss: 1.0983 - val_acc: 0.8660\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 1169s 58ms/step - loss: 0.0234 - acc: 0.9936 - val_loss: 0.7107 - val_acc: 0.8844\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 1161s 58ms/step - loss: 0.0168 - acc: 0.9953 - val_loss: 0.8909 - val_acc: 0.8814\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 1160s 58ms/step - loss: 0.0170 - acc: 0.9955 - val_loss: 1.1406 - val_acc: 0.8744\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 1166s 58ms/step - loss: 0.0159 - acc: 0.9955 - val_loss: 0.9481 - val_acc: 0.8798\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 1161s 58ms/step - loss: 0.0168 - acc: 0.9960 - val_loss: 0.8851 - val_acc: 0.8802\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 1165s 58ms/step - loss: 0.0142 - acc: 0.9954 - val_loss: 0.9197 - val_acc: 0.8788\n",
      "Epoch 16/20\n",
      "20000/20000 [==============================] - 1164s 58ms/step - loss: 0.0117 - acc: 0.9973 - val_loss: 1.0907 - val_acc: 0.8716\n",
      "Epoch 17/20\n",
      "20000/20000 [==============================] - 1164s 58ms/step - loss: 0.0114 - acc: 0.9971 - val_loss: 1.0945 - val_acc: 0.8856\n",
      "Epoch 18/20\n",
      "20000/20000 [==============================] - 1160s 58ms/step - loss: 0.0163 - acc: 0.9972 - val_loss: 1.6064 - val_acc: 0.8462\n",
      "Epoch 19/20\n",
      "20000/20000 [==============================] - 1161s 58ms/step - loss: 0.0131 - acc: 0.9974 - val_loss: 1.0702 - val_acc: 0.8790\n",
      "Epoch 20/20\n",
      "20000/20000 [==============================] - 1165s 58ms/step - loss: 0.0151 - acc: 0.9975 - val_loss: 1.1699 - val_acc: 0.8796\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f26d5446470>"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=50,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "apCPWvZz0S57"
   },
   "source": [
    "<a id='BiLSTM'></a>\n",
    "\n",
    "## Birdirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1696
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8981214,
     "status": "error",
     "timestamp": 1535444919924,
     "user": {
      "displayName": "Kok Meng Tan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "113459036002015692798"
     },
     "user_tz": -480
    },
    "id": "6UPQXYw80S58",
    "outputId": "c1f020be-5939-4630-88ea-03cc9855c0cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - Bidirectional LSTM\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 1000, 200)         4000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 4,241,202\n",
      "Trainable params: 4,241,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 3451s 138ms/step - loss: 0.4309 - acc: 0.8122 - val_loss: 0.3848 - val_acc: 0.8410\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 3420s 137ms/step - loss: 0.2655 - acc: 0.8979 - val_loss: 0.4057 - val_acc: 0.8426\n",
      "Epoch 3/10\n",
      "18800/25000 [=====================>........] - ETA: 11:32 - loss: 0.2076 - acc: 0.9240"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7591613603ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m model.fit(x_train, y_train, validation_data=(x_val, y_val),\n\u001b[0;32m---> 15\u001b[0;31m           epochs=10, batch_size=50)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH, ), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "l_lstm = Bidirectional(LSTM(units=100))(embedded_sequences)\n",
    "preds = Dense(units=2, activation='softmax')(l_lstm)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"model fitting - Bidirectional LSTM\")\n",
    "print(model.summary())\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=10,\n",
    "          batch_size=50,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vs8A-FyO0S5_"
   },
   "source": [
    "<a id='AttGRU'></a>\n",
    "\n",
    "## Attention GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "326P2x3X0S6A"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras import activations, initializers, regularizers, constraints\n",
    "from keras.layers import Layer, InputSpec\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], ),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1], ),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "        super(Attention, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim\n",
    "    \n",
    "    \n",
    "# class AttLayer(Layer):\n",
    "#     def __init__(self, **kwargs):\n",
    "#         self.init = initializations.get('normal')\n",
    "#         #self.input_spec = [InputSpec(ndim=3)]\n",
    "#         super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         assert len(input_shape)==3\n",
    "#         #self.W = self.init((input_shape[-1],1))\n",
    "#         self.W = self.init((input_shape[-1],))\n",
    "#         #self.input_spec = [InputSpec(shape=input_shape)]\n",
    "#         self.trainable_weights = [self.W]\n",
    "#         super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "#     def call(self, x, mask=None):\n",
    "#         eij = K.tanh(K.dot(x, self.W))\n",
    "        \n",
    "#         ai = K.exp(eij)\n",
    "#         weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "        \n",
    "#         weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "#         return weighted_input.sum(axis=1)\n",
    "\n",
    "#     def get_output_shape_for(self, input_shape):\n",
    "#         return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1696
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2840361,
     "status": "error",
     "timestamp": 1535447836510,
     "user": {
      "displayName": "Kok Meng Tan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "113459036002015692798"
     },
     "user_tz": -480
    },
    "id": "4_sIJt_K0S6E",
    "outputId": "9f3cd1b0-ffc7-4b98-ebd7-525e7c27e564"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - attention GRU network\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 1000, 200)         4000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 1000, 200)         180600    \n",
      "_________________________________________________________________\n",
      "attention_1 (Attention)      (None, 200)               1200      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 4,182,202\n",
      "Trainable params: 4,182,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 2744s 110ms/step - loss: 0.4212 - acc: 0.8081 - val_loss: 0.2976 - val_acc: 0.8748\n",
      "Epoch 2/10\n",
      " 1000/25000 [>.............................] - ETA: 35:40 - loss: 0.2486 - acc: 0.9140"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-7794aa0ac67a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m model.fit(x_train, y_train, validation_data=(x_val, y_val),\n\u001b[0;32m---> 17\u001b[0;31m           epochs=10, batch_size=50)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH, ), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "l_gru = Bidirectional(GRU(units=100, return_sequences=True))(embedded_sequences)\n",
    "l_attn = Attention(MAX_SEQUENCE_LENGTH)(l_gru)\n",
    "preds = Dense(units=2, activation='softmax')(l_attn)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"model fitting - attention GRU network\")\n",
    "print(model.summary())\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=10, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r2KNqH5i0S6G"
   },
   "outputs": [],
   "source": [
    "model.save_weights('models/AttGRU_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2EExzRt_0S6K"
   },
   "source": [
    "<a id='H-LSTM'></a>\n",
    "\n",
    "## Hierarchical LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RlTYML7P0S6L"
   },
   "source": [
    "Need to construct the data input as 3D other than 2D in previous two posts. So the input tensor would be (# of reviews each batch, # of sentences, # of words in each sentence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H1lRCIOS0S6L",
    "outputId": "0fd85ea4-64ee-4560-a7b3-16c585d95761"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/kokmeng/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SMnLCqIc0S6O"
   },
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \"\", string)\n",
    "    string = re.sub(r\"\\'\", \"\", string)\n",
    "    string = re.sub(r\"\\\"\", \"\", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "data_train = pd.read_csv(DIRNAME + 'data/labeledTrainData.tsv', sep='\\t')\n",
    "texts = []\n",
    "reviews = []\n",
    "labels = []\n",
    "for i in range(data_train.review.shape[0]):\n",
    "    text = clean_str(BeautifulSoup(data_train.review[i], 'html5lib').get_text())\n",
    "    texts.append(text)\n",
    "    reviews.append(tokenize.sent_tokenize(text))\n",
    "    labels.append(data_train.sentiment[i])\n",
    "    \n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uqTjJUeb0S6T",
    "outputId": "8683a9f9-8754-4cec-fea4-96211d2750d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (25000, 15, 100)\n",
      "Found 81503 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "MAX_SENT_LENGTH = 100\n",
    "MAX_SENTS = 15\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "data = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "for i, sentences in enumerate(reviews):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j < MAX_SENTS:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            #set max number of words\n",
    "            k = 0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                if k < MAX_SENT_LENGTH and tokenizer.word_index[word] < MAX_NUM_WORDS:\n",
    "                    data[i,j,k] = tokenizer.word_index[word]\n",
    "                    k = k + 1\n",
    "print('Shape of data tensor:', data.shape)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cECx-Lhu0S6W",
    "outputId": "258a123d-bb2c-4625-b4ab-b1ea56d59a68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive and negative reviews in traing and validation set \n",
      "[ 9953. 10047.]\n",
      "[2547. 2453.]\n"
     ]
    }
   ],
   "source": [
    "# Shuffling and splitting into train and validation sets\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n",
    "print('Number of positive and negative reviews in training and validation set')\n",
    "print(y_train.sum(axis=0))\n",
    "print(y_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = glove_embedding_matrix(EMBEDDING_DIM, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "76T4XBXK0S6c"
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(\n",
    "    len(word_index) + 1,\n",
    "    EMBEDDING_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=MAX_SENT_LENGTH,\n",
    "    trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qWpe0VSr0S6e",
    "outputId": "1f2bae90-8712-425d-b2f9-f69d0ab5a7c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - Hierachical LSTM\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 15, 100)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 15, 200)           8311200   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 8,552,402\n",
      "Trainable params: 8,552,402\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 973s 49ms/step - loss: 0.5081 - acc: 0.7454 - val_loss: 0.3353 - val_acc: 0.8538\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 992s 50ms/step - loss: 0.2956 - acc: 0.8793 - val_loss: 0.2870 - val_acc: 0.8800\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 972s 49ms/step - loss: 0.2216 - acc: 0.9145 - val_loss: 0.2784 - val_acc: 0.8874\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 1033s 52ms/step - loss: 0.1753 - acc: 0.9336 - val_loss: 0.3474 - val_acc: 0.8784\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 1012s 51ms/step - loss: 0.1369 - acc: 0.9503 - val_loss: 0.2654 - val_acc: 0.9010\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 958s 48ms/step - loss: 0.1030 - acc: 0.9627 - val_loss: 0.3230 - val_acc: 0.9010\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 1019s 51ms/step - loss: 0.0733 - acc: 0.9736 - val_loss: 0.3118 - val_acc: 0.8954\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 983s 49ms/step - loss: 0.0492 - acc: 0.9835 - val_loss: 0.3754 - val_acc: 0.8926\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 952s 48ms/step - loss: 0.0314 - acc: 0.9902 - val_loss: 0.5426 - val_acc: 0.8970\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 945s 47ms/step - loss: 0.0187 - acc: 0.9951 - val_loss: 0.5067 - val_acc: 0.8960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd72874fe80>"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "\n",
    "l_lstm = Bidirectional(LSTM(units=100))(embedded_sequences)\n",
    "sentEncoder = Model(sentence_input, l_lstm)\n",
    "\n",
    "review_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "\n",
    "l_lstm_sent = Bidirectional(LSTM(units=100))(review_encoder)\n",
    "preds = Dense(units=2, activation='softmax')(l_lstm_sent)\n",
    "\n",
    "model = Model(review_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"model fitting - Hierachical LSTM\")\n",
    "print(model.summary())\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=10, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fOIOAKWt0S6i"
   },
   "outputs": [],
   "source": [
    "model.load_weights('models/HLSTM_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_egM5LQu0S6k"
   },
   "source": [
    "<a id='HAN'></a>\n",
    "\n",
    "## Hierarchical Attention Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0i-J5erm0S6k"
   },
   "outputs": [],
   "source": [
    "# x = TimeDistributed(Dense(300), input_shape=(MAX_SEQUENCE_LENGTH, 392))(concated)\n",
    "# x = TimeDistributed(Activation('tanh'))(x)\n",
    "# x = TimeDistributed(Dense(1))(x)\n",
    "# x = Flatten()(x)\n",
    "# x = Activation('softmax')(x)\n",
    "# x = Reshape((-1,1))(x)\n",
    "# final_model = merge([concated, x], mode = lambda x: K.batch_dot(x[0], x[1], axes=[1,1]),\n",
    "#                     output_shape= lambda x: (x[0][0], x[0][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zB9sPXx40S6m",
    "outputId": "d804c508-50c7-42f1-f17a-346f0e8d6050"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - Hierachical attention network\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 15, 100)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 15, 200)           8311500   \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 15, 200)           180600    \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 15, 200)           40200     \n",
      "_________________________________________________________________\n",
      "attention_3 (Attention)      (None, 200)               215       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 8,532,917\n",
      "Trainable params: 8,532,917\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 1146s 57ms/step - loss: 0.5529 - acc: 0.7036 - val_loss: 0.3150 - val_acc: 0.8708\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 1101s 55ms/step - loss: 0.3099 - acc: 0.8696 - val_loss: 0.2700 - val_acc: 0.8938\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 1106s 55ms/step - loss: 0.2429 - acc: 0.9016 - val_loss: 0.2602 - val_acc: 0.8996\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 1121s 56ms/step - loss: 0.1945 - acc: 0.9238 - val_loss: 0.2548 - val_acc: 0.8996\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 1150s 58ms/step - loss: 0.1547 - acc: 0.9425 - val_loss: 0.2512 - val_acc: 0.9066\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 1599s 80ms/step - loss: 0.1211 - acc: 0.9555 - val_loss: 0.2679 - val_acc: 0.8970\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 1634s 82ms/step - loss: 0.0903 - acc: 0.9674 - val_loss: 0.2990 - val_acc: 0.9042\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 1640s 82ms/step - loss: 0.0624 - acc: 0.9794 - val_loss: 0.3240 - val_acc: 0.9030\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 1620s 81ms/step - loss: 0.0393 - acc: 0.9872 - val_loss: 0.3660 - val_acc: 0.8998\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 1640s 82ms/step - loss: 0.0257 - acc: 0.9930 - val_loss: 0.4281 - val_acc: 0.8966\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa49d2c6278>"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "\n",
    "l_gru = Bidirectional(GRU(units=100, return_sequences=True))(embedded_sequences)\n",
    "l_dense = TimeDistributed(Dense(units=200))(l_gru)\n",
    "l_attn = Attention(MAX_SENT_LENGTH)(l_dense)\n",
    "sentEncoder = Model(sentence_input, l_attn)\n",
    "\n",
    "review_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "\n",
    "l_gru_sent = Bidirectional(GRU(units=100, return_sequences=True))(review_encoder)\n",
    "l_dense_sent = TimeDistributed(Dense(units=200))(l_gru_sent)\n",
    "l_attn_sent = Attention(MAX_SENTS)(l_dense_sent)\n",
    "preds = Dense(units=2, activation='softmax')(l_attn_sent)\n",
    "\n",
    "model = Model(review_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"model fitting - Hierachical attention network\")\n",
    "print(model.summary())\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=10, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KaHpAcwA0S6o"
   },
   "outputs": [],
   "source": [
    "model.save_weights('models/han_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "evB_X3yX0S6r"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "keras_textclassifiers.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
